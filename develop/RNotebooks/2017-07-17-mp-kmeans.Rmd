---
title: "Analysing Bee Features Data Set"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---

Load necessary libraries & sources
```{r}
library(data.table)
library(stringr)
library(ggplot2)
library(here)

source(here("src/R/helper_functions.R"))
```

Load data into memory and check that it doesn't contain any NA's
```{r}
dt <- fread(here("Output_features/segment_features.csv"),
            stringsAsFactors = TRUE)

print(any(is.na(dt)))

dt_xy <- fread(here("Data/Output_features/segment_xys.csv"), head = TRUE)
```

Lets take a look at the data and see if the ranges look alright
```{r}
summary(dt)
```

3.06469 is a little large and a considerable outlier for InnerRadiusVariation
```{r}
hist(dt$InnerRadiusVariation)
```

I'm not sure what to do with this though. Is a value this high possible?

There are also two MeanAbsRotation columns with identical values. I checked the python code and found a bug in it that created this. Unfortunately it will take a while to run it and so I have left this for now (although I have made a note).


Min and Max Rotation contain some quite weird high valued outliers. 3.9 indicates approximately 180 degree turn. I think this could be caused by the bee moving backwards, at least this seems like the only sensible explanation for now other than an error in the measurements. Lets leave them in for now because they might help define a cluster
```{r}
summary(dt[, .(MinRotation, MaxRotation, MaxAbsRotation)])
```

SumAbsolteAngles contains an extremely large outliers
```{r}
hist(dt$SumAbsoluteAngles, breaks = 50)
```

```{r}
dt[SumAbsoluteAngles>4000, .N]
```
```{r}
dt[SumAbsoluteAngles>6000, .N]
```
```{r}
dt[SumAbsoluteAngles>8000, .N]
```
Again not sure what is going on here We might need to come back to this and think about it, but for now in this preliminary study I will ignore this.

Since I believe it unlikely that speed will determine the appearance of a segment for now we will exclude it.

```{r}
dt_names <- names(dt)
speed_names <- str_subset(dt_names, "Speed")
dt_mod <- dt[, (speed_names) := NULL]
summary(dt_mod)
```

Lets see what happens if we perform kmeans now.

There is likely a lot of correlation of variables. To improve efficiency of calculating the kmeans I will use PCA

```{r}
pr.out <- prcomp(dt_mod[, MedianDistanceFromCentre:SumAbsoluteAngles], scale = TRUE)
print(pr.out)
```
```{r}
summary(pr.out)
```

Lets see if we can see an elbow to determine the number of components to use:
```{r}
var <- pr.out$sdev^2
pve <- var / sum(var)
plot(pve)
```



Looks like the first 2 components will be pretty good. lets take a look at what it looks like in 2-d plot.

```{r}
#plot(pr.out$x$PC1, pr.out$x$PC2)
# head(pr.out$x)
plot(pr.out$x[, "PC1"], pr.out$x[, "PC2"], pch = ".")
```

Maybe my lack of experience is showing here, but I'm not sure this helps us identify discrete clusters. What does it look like plotted as a histogram in 1-dimension?

```{r}
hist(pr.out$x[, "PC1"], breaks = 200)
```


Perhaps looking at the Total within SS values for different numbers of clusters and checking for an elbow might be a good way to select k.

```{r}
wss <- 0
for(i in 1:15){
  km.out <- kmeans(x = pr.out$x[, 1:2], centers = i, nstart = 20, algorithm = "Lloyd", iter.max = 200)
  wss[i] <- km.out$tot.withinss
}
plot(x = 1:15, wss, type = "b", xlab = "Number of clusters", ylab = "Total within sum of squares", ylim = c(0, 150000))

```

3 is possibly where the elbow is. What happens if we try a 3 cluster model?
```{r}
km.out <- kmeans(x = pr.out$x[, 1:2], centers = 3, nstart = 20)
plot(pr.out$x[, "PC1"], pr.out$x[, "PC2"], col = km.out$cluster, pch = ".")
dt[, cluster := km.out$cluster]
```

Now its time to start looking at some of the trajectories for each of the clusters to see if there is any differentiation between the clusters.

# Using 2 principal components and 3 clusters

```{r, fig.width=5,fig.height=10}
set.seed(1) #the following code take a random sample of the segments
plot_cluster(features = dt, xy = dt_xy, nSegments = 5)
```

# Using 2 principal components and 6 clusters

```{r, fig.width=10,fig.height=10}
set.seed(1) #the following code take a random sample of the segments
km.out <- kmeans(x = pr.out$x[, 1:2], centers = 6, nstart = 20)
dt[, cluster := km.out$cluster]
plot_cluster(features = dt, xy = dt_xy, nSegments = 5)
```



Let's go back to the principal components and use the first 5 because they represent >70% variance and also the component loadings within these components make reasonable sense i.e. each components loadings are grouped by similar meanings.
```{r}
wss <- 0
for(i in 1:30){
  km.out <- kmeans(x = pr.out$x[, 1:5], centers = i, nstart = 20, algorithm = "Lloyd", iter.max = 200)
  wss[i] <- km.out$tot.withinss
  cat("Finished model ", i)
}
```
What happens if we plot what the total within sum of squares is for different numbers of clusters:
```{r}
plot(x = 1:30, wss, type = "b", xlab = "Number of clusters", ylab = "Total within sum of squares", ylim = c(0, max(wss)))
```


# Using 5 principal components and 3 clusters

```{r, fig.width=5,fig.height=10}
set.seed(1) #the following code take a random sample of the segments
km.out <- kmeans(x = pr.out$x[, 1:5], centers = 3, nstart = 20)
dt[, cluster := km.out$cluster]
plot_cluster(features = dt, xy = dt_xy, nSegments = 5)
```

# Using 5 principal components and 6 clusters

There seems to be different segments within clusters. Lets see what happens if we increase the number of clusters to 6:
```{r, fig.width=10,fig.height=8}
set.seed(1) #the following code take a random sample of the segments
km.out <- kmeans(x = pr.out$x[, 1:5], centers = 6, nstart = 20)
dt[, cluster := km.out$cluster]
plot_cluster(features = dt, xy = dt_xy, nSegments = 5)
```

Lets also look at results of Kmeans without applying PCA

# No PCA 3 Clusters
```{r, fig.width=5,fig.height=10}
set.seed(1) #the following code take a random sample of the segments
km.out <- kmeans(x = scale(dt_mod[, MedianDistanceFromCentre:SumAbsoluteAngles]), centers = 3, nstart = 20)
dt_mod[, cluster := km.out$cluster]
plot_cluster(features = dt_mod, xy = dt_xy, nSegments = 5)

```

# No PCA 6 Clusters
```{r, fig.width=10,fig.height=10}
set.seed(1) #the following code take a random sample of the segments
km.out <- kmeans(x = scale(dt_mod[, MedianDistanceFromCentre:SumAbsoluteAngles]), centers = 6, nstart = 20)
dt_mod[, cluster := km.out$cluster]
plot_cluster(features = dt_mod, xy = dt_xy, nSegments = 5)
```


It seems to me like we are better off without the use of PCA. What would be the main purpose of using PCA here anyway?

#Removing the rotation features

I have a suspicion that these features are not helping us to identify shapes in the segment paths. Lets see what happens if we remove them.

```{r, fig.width=10,fig.height=10}
set.seed(1) #the following code take a random sample of the segments

features_only <- names(dt_mod)[-(1:4)]
rotation_features <- str_subset(features_only, "Rotation")
dt_no_rotate <- dt_no_rotate[, !(rotation_features), with = F]
km.out <- kmeans(x = scale(dt_no_rotate[, MedianDistanceFromCentre:SumAbsoluteAngles]), centers = 6, nstart = 20)
dt_no_rotate[, cluster := km.out$cluster]
plot_cluster(features = dt_no_rotate, xy = dt_xy, nSegments = 5)
```

Three clusters
```{r, fig.width=5,fig.height=10}
set.seed(1) #the following code take a random sample of the segments

features_only <- names(dt_mod)[-(1:4)]
rotation_features <- str_subset(features_only, "Rotation")
dt_no_rotate <- dt_no_rotate[, !(rotation_features), with = F]
km.out <- kmeans(x = scale(dt_no_rotate[, MedianDistanceFromCentre:SumAbsoluteAngles]), centers = 3, nstart = 20)
dt_no_rotate[, cluster := km.out$cluster]
plot_cluster(features = dt_no_rotate, xy = dt_xy, nSegments = 5)
```

